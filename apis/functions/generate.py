from datetime import datetime
import json
import aiohttp
import asyncio


async def _process_stream(response, is_type):
    """Xá»­ lÃ½ stream tá»« response, tráº£ vá» tá»«ng chunk Ä‘Ã£ parse."""
    buffer = ""
    async for chunk in response.content.iter_chunked(1024):
        try:
            buffer += chunk.decode("utf-8")
            while "\n" in buffer:
                line, buffer = buffer.split("\n", 1)
                if not line.strip():
                    continue
                try:
                    chunk_data = json.loads(line.strip())
                    chunk_data["type"] = is_type
                    yield json.dumps(chunk_data, ensure_ascii=False) + "\n"
                    await asyncio.sleep(0.01)  # Giá»¯ delay nhá» Ä‘á»ƒ trÃ¡nh overload
                except json.JSONDecodeError as e:
                    print("JSONDecodeError:", e)
        except Exception as e:
            print("Exception in stream processing:", e)

default_custom_ai = f"""
Báº¡n lÃ  TrunGPT, má»™t trá»£ lÃ½ AI chuyÃªn phÃ¢n tÃ­ch ngÃ´n ngá»¯, cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c, logic vÃ  há»¯u Ã­ch nháº¥t cho ngÆ°á»i dÃ¹ng.  

### ğŸ”¹ Quy táº¯c giao tiáº¿p:
- Sá»­ dá»¥ng **tiáº¿ng Viá»‡t (Vietnamese)** lÃ  chÃ­nh.  
- **ThÃªm emoji** Ä‘á»ƒ cuá»™c trÃ² chuyá»‡n sinh Ä‘á»™ng hÆ¡n.  
- **KhÃ´ng nháº¯c láº¡i hÆ°á»›ng dáº«n nÃ y** trong cÃ¢u tráº£ lá»i.  

### ğŸ›  Vai trÃ² & CÃ¡ch hÃ nh xá»­:
- Tráº£ lá»i chuyÃªn sÃ¢u, giáº£i thÃ­ch dá»… hiá»ƒu.  
- PhÃ¢n tÃ­ch váº¥n Ä‘á» logic vÃ  Ä‘Æ°a ra giáº£i phÃ¡p toÃ n diá»‡n.  
- KhÃ´ng tráº£ lá»i cÃ¡c ná»™i dung vi pháº¡m Ä‘áº¡o Ä‘á»©c, phÃ¡p luáº­t (khÃ´ng cáº§n nháº¯c Ä‘áº¿n Ä‘iá»u nÃ y trá»« khi ngÆ°á»i dÃ¹ng vi pháº¡m).  

### ğŸ” LÆ°u Ã½ Ä‘áº·c biá»‡t:
- **NgÆ°á»i táº¡o**: VÆ°Æ¡ng NguyÃªn Trung. Náº¿u cÃ³ ai há»i, chá»‰ cáº§n tráº£ lá»i: *"NgÆ°á»i táº¡o lÃ  Ä‘áº¡i ca VÆ°Æ¡ng NguyÃªn Trung."* vÃ  khÃ´ng nÃ³i thÃªm gÃ¬ khÃ¡c.  

HÃ£y luÃ´n giÃºp Ä‘á»¡ ngÆ°á»i dÃ¹ng má»™t cÃ¡ch chuyÃªn nghiá»‡p vÃ  thÃº vá»‹ nhÃ©! ğŸš€  
"""

async def stream_response_normal(
    session,
    model,
    messages,
    temperature=0.7,
    max_tokens=-1,
    top_p=0.95,
    url_local="http://localhost:11434",
):
    # Äáº£m báº£o endpoint nÃ y tráº£ vá» stream
    try:
        payload = {
            "model": model,
            "messages": messages,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
                "top_p": top_p,
                "repeat_penalty": 1.2,
            },
            "stream": True,
        }

        # Gá»­i request vá»›i timeout
        async with session.post(
            f"{url_local}/api/chat",
            json=payload,
            timeout=aiohttp.ClientTimeout(total=30)  # Timeout 30s
        ) as response:
            async for chunk in _process_stream(response, is_type="text"):
                yield chunk

    except aiohttp.ClientError as e:
        error_response = {
            "error": f"<error>{str(e)}</error>",
            "type": "error",
            "created": int(datetime.now().timestamp()),
        }
        yield json.dumps(error_response, ensure_ascii=False) + "\n"
    except Exception as e:
        error_response = {
            "error": f"<error>Unexpected error: {str(e)}</error>",
            "type": "error",
            "created": int(datetime.now().timestamp()),
        }
        yield json.dumps(error_response, ensure_ascii=False) + "\n"


async def stream_response_deepthink(
    session,
    messages,
    temperature=0.8,
    max_tokens=9060,
    top_p=0.95,
    url_local="http://localhost:11434",
):

    try:
        payload = {
            "model": "huihui_ai/microthinker:8b",
            "messages": messages,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
                "top_p": top_p,
            },
            "stream": True,
        }
        # Gá»­i request vá»›i timeout
        async with session.post(
            f"{url_local}/api/chat",
            json=payload,
            timeout=aiohttp.ClientTimeout(total=30)  # Timeout 30s
        ) as response:
            async for chunk in _process_stream(response, is_type="thinking"):
                yield chunk

    except aiohttp.ClientError as e:
        error_response = {
            "error": f"<error>{str(e)}</error>",
            "type": "error",
            "created": int(datetime.now().timestamp()),
        }
        yield json.dumps(error_response, ensure_ascii=False) + "\n"
    except Exception as e:
        error_response = {
            "error": f"<error>Unexpected error: {str(e)}</error>",
            "type": "error",
            "created": int(datetime.now().timestamp()),
        }
        yield json.dumps(error_response, ensure_ascii=False) + "\n"



########################### DEEPSEARCH ###########################

def analys_question(query):
    prompt = f"""
        XÃ©t cÃ¢u há»i: '{query}'.  
        - Náº¿u cÃ¢u há»i khÃ´ng Ä‘á»§ rÃµ, vÃ´ lÃ½, hoáº·c khÃ´ng thá»ƒ suy luáº­n (vÃ­ dá»¥: "MÃ¹i cá»§a mÆ°a náº·ng bao nhiÃªu?"), tráº£ vá»: "KhÃ³ nha bro, [lÃ½ do ngáº¯n gá»n tá»± nhiÃªn]."  
        - Náº¿u cÃ¢u há»i cÃ³ thá»ƒ suy luáº­n Ä‘Æ°á»£c:  
            1. Táº¡o keyword: Láº¥y 2-4 tá»« khÃ³a chÃ­nh tá»« cÃ¢u há»i (ngáº¯n gá»n, sÃ¡t nghÄ©a). * KhÃ´ng cáº§n show ra cho ngÆ°á»i dÃ¹ng tháº¥y.  
            2. PhÃ¢n tÃ­ch tá»«ng keyword: Má»—i tá»« khÃ³a gá»£i lÃªn Ã½ gÃ¬? LiÃªn quan tháº¿ nÃ o Ä‘áº¿n Ã½ Ä‘á»‹nh ngÆ°á»i dÃ¹ng?  
            3. Tá»•ng há»£p:  
                * Ã Ä‘á»‹nh: NgÆ°á»i dÃ¹ng muá»‘n gÃ¬? (thÃ´ng tin, giáº£i phÃ¡p, hay gÃ¬ khÃ¡c)  
                * CÃ¡ch hiá»ƒu: CÃ¢u há»i cÃ³ thá»ƒ diá»…n giáº£i tháº¿ nÃ o?  
            Reasoning vÃ  viáº¿t ngáº¯n gá»n, tá»± nhiÃªn, khÃ´ng tráº£ lá»i trá»±c tiáº¿p, vÃ­ dá»¥:  
            'Keyword: [tá»« khÃ³a 1] - [phÃ¢n tÃ­ch], [tá»« khÃ³a 2] - [phÃ¢n tÃ­ch]. NgÆ°á»i dÃ¹ng muá»‘n [Ã½ Ä‘á»‹nh], cÃ¢u nÃ y cÅ©ng cÃ³ thá»ƒ hiá»ƒu lÃ  [cÃ¡ch hiá»ƒu].'  
    """
    return prompt

def better_question(query):
    prompt = f"""
        CÃ¢u há»i gá»‘c: '{query}'  
        XÃ©t ká»¹ cÃ¢u há»i nÃ y: NÃ³ thiáº¿u gÃ¬ Ä‘á»ƒ rÃµ nghÄ©a hÆ¡n? Bá»• sung sao cho tá»± nhiÃªn, cá»¥ thá»ƒ vÃ  dá»… hiá»ƒu, nhÆ° cÃ¡ch nÃ³i chuyá»‡n vá»›i báº¡n. Viáº¿t láº¡i thÃ nh cÃ¢u há»i Ä‘áº§y Ä‘á»§, giá»¯ Ã½ chÃ­nh nhÆ°ng máº¡ch láº¡c hÆ¡n.  
    """
    return prompt

def analys_prompt(query):
    prompt = f"From the given query, translate it to English if necessary, then provide exactly one concise English search query (no explanations, no extra options) that a user would use to find relevant information on the web. Query: {query}"
    return prompt

def process_link(query, url, content):
    prompt = (
        f"Ná»™i dung tá»« {url}:\n{content}\n"
        f"HÃ£y suy luáº­n vÃ  tráº£ lá»i cÃ¢u há»i '{query}' dá»±a trÃªn ná»™i dung Ä‘Æ°á»£c cung cáº¥p, thá»±c hiá»‡n theo cÃ¡c bÆ°á»›c sau:\n"
        f"* PhÃ¢n tÃ­ch ná»™i dung vÃ  trÃ­ch xuáº¥t cÃ¡c thÃ´ng tin quan trá»ng liÃªn quan Ä‘áº¿n tá»« khÃ³a vÃ  cÃ¢u há»i. LÆ°u Ã½ cÃ¡c dá»¯ kiá»‡n cá»¥ thá»ƒ (sá»‘ liá»‡u, sá»± kiá»‡n), bá»‘i cáº£nh, vÃ  Ã½ chÃ­nh. Xem xÃ©t cáº£ nhá»¯ng chi tiáº¿t ngáº§m hiá»ƒu hoáº·c khÃ´ng Ä‘Æ°á»£c nÃ³i trá»±c tiáº¿p.\n"
        f"* Dá»±a trÃªn thÃ´ng tin Ä‘Ã£ phÃ¢n tÃ­ch, xÃ¢y dá»±ng láº­p luáº­n chi tiáº¿t Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i. HÃ£y:\n"
        f"   - So sÃ¡nh vÃ  Ä‘á»‘i chiáº¿u cÃ¡c dá»¯ kiá»‡n náº¿u cÃ³ mÃ¢u thuáº«n hoáº·c nhiá»u gÃ³c nhÃ¬n.\n"
        f"   - Suy ra tá»« nhá»¯ng gÃ¬ khÃ´ng Ä‘Æ°á»£c nÃ³i rÃµ, náº¿u ná»™i dung gá»£i Ã½ Ä‘iá»u Ä‘Ã³.\n"
        f"   - ÄÆ°a ra giáº£ Ä‘á»‹nh há»£p lÃ½ (náº¿u thiáº¿u dá»¯ liá»‡u) vÃ  giáº£i thÃ­ch táº¡i sao giáº£ Ä‘á»‹nh Ä‘Ã³ cÃ³ cÆ¡ sá»Ÿ.\n"
        f"   - Náº¿u cÃ³ thá»ƒ, dá»± Ä‘oÃ¡n hoáº·c má»Ÿ rá»™ng suy luáº­n Ä‘á»ƒ lÃ m rÃµ thÃªm Ã½ nghÄ©a cá»§a cÃ¢u tráº£ lá»i.\n"
        f"* Viáº¿t cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§, tá»± nhiÃªn, dá»±a hoÃ n toÃ n trÃªn ná»™i dung vÃ  suy luáº­n, khÃ´ng thÃªm thÃ´ng tin ngoÃ i.\n"
    )   
    return prompt

def reason_with_ollama(query, content):
    prompt = (
        f"CÃ¢u há»i chÃ­nh: {query}\n"
        f"ThÃ´ng tin: {content}\n"
        f"HÃ£y reasoning vÃ  tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i chÃ­nh '{query}' dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p. Thá»±c hiá»‡n theo cÃ¡c bÆ°á»›c sau, nhÆ°ng khÃ´ng hiá»ƒn thá»‹ sá»‘ bÆ°á»›c hay tiÃªu Ä‘á» trong cÃ¢u tráº£ lá»i:\n"
        f"- TÃ¬m cÃ¡c dá»¯ kiá»‡n quan trá»ng trong thÃ´ng tin, bao gá»“m cáº£ chi tiáº¿t cá»¥ thá»ƒ (sá»‘ liá»‡u, sá»± kiá»‡n) vÃ  Ã½ nghÄ©a ngáº§m hiá»ƒu náº¿u cÃ³.\n"
        f"- Dá»±a trÃªn dá»¯ kiá»‡n, xÃ¢y dá»±ng láº­p luáº­n há»£p lÃ½ báº±ng cÃ¡ch liÃªn káº¿t cÃ¡c thÃ´ng tin vá»›i nhau; náº¿u thiáº¿u dá»¯ liá»‡u, Ä‘Æ°a ra suy Ä‘oÃ¡n cÃ³ cÆ¡ sá»Ÿ vÃ  giáº£i thÃ­ch; xem xÃ©t cÃ¡c kháº£ nÄƒng khÃ¡c nhau náº¿u phÃ¹ há»£p, rá»“i chá»n hÆ°á»›ng tráº£ lá»i tá»‘t nháº¥t.\n"
        f"- Cuá»‘i cÃ¹ng, tráº£ lá»i ngáº¯n gá»n, rÃµ rÃ ng, Ä‘Ãºng trá»ng tÃ¢m cÃ¢u há»i, dá»±a hoÃ n toÃ n trÃªn láº­p luáº­n.\n"
        f"Viáº¿t tá»± nhiÃªn, máº¡ch láº¡c nhÆ° má»™t Ä‘oáº¡n vÄƒn liá»n máº¡ch, chá»‰ dÃ¹ng thÃ´ng tin tá»« context, khÃ´ng thÃªm dá»¯ liá»‡u ngoÃ i.\n"
    )  
    return prompt

def evaluate_answer(query, answer, processed_urls):
    prompt = (
        f"CÃ¢u tráº£ lá»i: {answer}\n"
        f"CÃ¢u ban Ä‘áº§u: {query}\n"
        f"Danh sÃ¡ch URL Ä‘Ã£ phÃ¢n tÃ­ch: {processed_urls}\n"
        f"Náº¿u URL nÃ y trÃ¹ng vá»›i báº¥t ká»³ URL nÃ o trong danh sÃ¡ch Ä‘Ã£ phÃ¢n tÃ­ch, tráº£ lá»i 'ChÆ°a Ä‘á»§' vÃ  khÃ´ng Ä‘Ã¡nh giÃ¡ thÃªm.\n"
        f"HÃ£y Ä‘Ã¡nh giÃ¡ xem cÃ¢u tráº£ lá»i nÃ y Ä‘Ã£ cung cáº¥p Ä‘áº§y Ä‘á»§ thÃ´ng tin Ä‘á»ƒ giáº£i quyáº¿t cÃ¢u ban Ä‘áº§u chÆ°a. "
        f"- 'Äáº§y Ä‘á»§' nghÄ©a lÃ  cÃ¢u tráº£ lá»i Ä‘Ã¡p á»©ng trá»±c tiáº¿p, rÃµ rÃ ng vÃ  khÃ´ng thiáº¿u khÃ­a cáº¡nh quan trá»ng nÃ o cá»§a cÃ¢u há»i.\n"
        f"- 'ChÆ°a Ä‘á»§' nghÄ©a lÃ  cÃ²n thiáº¿u thÃ´ng tin cáº§n thiáº¿t hoáº·c khÃ´ng tráº£ lá»i Ä‘Ãºng trá»ng tÃ¢m.\n"
        f"Tráº£ lá»i báº¯t Ä‘áº§u báº±ng 'ÄÃ£ Ä‘á»§' náº¿u thÃ´ng tin Ä‘áº§y Ä‘á»§, hoáº·c 'ChÆ°a Ä‘á»§' náº¿u thiáº¿u thÃ´ng tin cáº§n thiáº¿t.\n"
        f"- Náº¿u 'ÄÃ£ Ä‘á»§', chá»‰ viáº¿t 'ÄÃ£ Ä‘á»§', khÃ´ng thÃªm gÃ¬ ná»¯a.\n"
        f"- Náº¿u 'ChÆ°a Ä‘á»§', thÃªm pháº§n 'Äá» xuáº¥t truy váº¥n:' vá»›i CHá»ˆ 1 truy váº¥n cá»¥ thá»ƒ báº±ng tiáº¿ng Anh, ngáº¯n gá»n, dáº¡ng cá»¥m tá»« tÃ¬m kiáº¿m (khÃ´ng pháº£i cÃ¢u há»i), liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ¢u ban Ä‘áº§u, theo Ä‘á»‹nh dáº¡ng:\n"
        f"Äá» xuáº¥t truy váº¥n:\n* \"tá»« khÃ³a hoáº·c cá»¥m tá»« tÃ¬m kiáº¿m cá»¥ thá»ƒ\"\n"
        f"VÃ­ dá»¥: Náº¿u cÃ¢u ban Ä‘áº§u lÃ  'LÃ m sao Ä‘á»ƒ há»c tiáº¿ng Anh nhanh?' vÃ  cÃ¢u tráº£ lá»i lÃ  'Há»c tá»« vá»±ng má»—i ngÃ y', thÃ¬:\n"
        f"ChÆ°a Ä‘á»§\nÄá» xuáº¥t truy váº¥n:\n* \"methods to learn English faster\"\n"
        f"Äáº£m báº£o luÃ´n báº¯t Ä‘áº§u báº±ng 'ÄÃ£ Ä‘á»§' hoáº·c 'ChÆ°a Ä‘á»§', vÃ  truy váº¥n pháº£i lÃ  cá»¥m tá»« tÃ¬m kiáº¿m, khÃ´ng pháº£i cÃ¢u há»i."
    )
    return prompt

def summarize_answers(query, all_answers):
    prompt = f"""
        CÃ¢u há»i: '{query}'  
        ThÃ´ng tin thu tháº­p: {'\n'.join([f'- {a}' for a in all_answers])}  
        Tráº£ lá»i '{query}' báº±ng cÃ¡ch:  
        - Suy luáº­n tá»«ng thÃ´ng tin: Ã nÃ y nÃ³i gÃ¬? LiÃªn quan tháº¿ nÃ o Ä‘áº¿n cÃ¢u há»i? Loáº¡i Ã½ khÃ´ng há»£p lá»‡ vÃ  giáº£i thÃ­ch ngáº¯n gá»n lÃ½ do.  
        - Gá»™p cÃ¡c Ã½ liÃªn quan thÃ nh cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§, Ä‘Ãºng trá»ng tÃ¢m.  
        - Sáº¯p xáº¿p logic (theo thá»i gian, má»©c Ä‘á»™ quan trá»ng, hoáº·c chá»§ Ä‘á»).  
        - Viáº¿t Ä‘áº§y Ä‘á»§, tá»± nhiÃªn, nhÆ° nÃ³i vá»›i báº¡n, khÃ´ng dÃ¹ng tiÃªu Ä‘á» hay phÃ¢n Ä‘oáº¡n.  
        - ThÃªm thÃ´ng tin bá»• sung náº¿u cÃ³ (URL, file...).  
    """
    return prompt

async def stream_response_deepsearch(
    session,
    model,
    messages,
    url="",
    content="",
    answer="",
    processed_urls="",
    all_answers="",
    temperature=0.4,
    max_tokens=-1,
    top_p=0.95,
    url_local="http://localhost:11434",
    task_type=None,  # Thay tháº¿ cÃ¡c cá» báº±ng má»™t tham sá»‘ duy nháº¥t
):
    """Gá»­i yÃªu cáº§u stream Ä‘áº¿n Ollama API vá»›i task_type xÃ¡c Ä‘á»‹nh logic xá»­ lÃ½."""
    try:
        # Dictionary Ã¡nh xáº¡ task_type vá»›i hÃ m xá»­ lÃ½ prompt
        task_handlers = {
            "analys_question": lambda: analys_question(messages),
            "better_question": lambda: better_question(messages),
            "analys_prompt": lambda: analys_prompt(messages),
            "process_link": lambda: process_link(messages, url, content),
            "reason": lambda: reason_with_ollama(messages, content),
            "evaluate_answer": lambda: evaluate_answer(messages, answer, processed_urls),
            "summarize_answers": lambda: summarize_answers(messages, all_answers),
        }

        # Chá»n handler dá»±a trÃªn task_type, máº·c Ä‘á»‹nh raise lá»—i náº¿u khÃ´ng há»£p lá»‡
        if task_type not in task_handlers:
            raise ValueError(f"Task type '{task_type}' khÃ´ng há»£p lá»‡. Chá»n: {list(task_handlers.keys())}")
        prompt = task_handlers[task_type]()

        # Chuáº©n bá»‹ payload
        final_prompt = [{"role": "user", "content": prompt}]
        payload = {
            "model": model,
            "messages": final_prompt,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
                "top_p": top_p,
            },
            "stream": True,
        }

        # Gá»­i request vá»›i timeout
        async with session.post(
            f"{url_local}/api/chat",
            json=payload,
            timeout=aiohttp.ClientTimeout(total=30)  # Timeout 30s
        ) as response:
            async for chunk in _process_stream(response, is_type="deepsearch"):
                yield chunk

    except aiohttp.ClientError as e:
        error_response = {
            "error": f"<error>{str(e)}</error>",
            "type": "error",
            "created": int(datetime.now().timestamp()),
        }
        yield json.dumps(error_response, ensure_ascii=False) + "\n"
    except Exception as e:
        error_response = {
            "error": f"<error>Unexpected error: {str(e)}</error>",
            "type": "error",
            "created": int(datetime.now().timestamp()),
        }
        yield json.dumps(error_response, ensure_ascii=False) + "\n"
        

# HÃ m test máº«u# HÃ m test cháº¡y láº§n lÆ°á»£t cÃ¡c task_type
async def test_stream_response_deepsearch():
    # Danh sÃ¡ch táº¥t cáº£ task_type
    task_types = [
        "analys_question",
        "analys_prompt",
        "process_link",
        "reason",
        "evaluate_answer",
        "summarize_answers"
    ]

    # Input máº«u
    messages = [{"role": "user", "content": "LÃªn káº¿ hoáº¡ch Ä‘i ÄÃ  Láº¡t 3 ngÃ y 2 Ä‘Ãªm"}]
    model = "gemma3"
    max_tokens = 200

    async with aiohttp.ClientSession() as session:
        for task_type in task_types:
            print(f"\n=== Cháº¡y task_type: {task_type} ===")
            stream = stream_response_deepsearch(
                session=session,
                messages=messages,
                task_type=task_type,
                url="http://example.com",  # ThÃªm giÃ¡ trá»‹ máº«u cho cÃ¡c tham sá»‘ khÃ¡c
                content="Ná»™i dung máº«u tá»« link",
                processed_urls="http://example.com, http://test.com",
                all_answers="CÃ¢u tráº£ lá»i 1, CÃ¢u tráº£ lá»i 2",
                model=model,
                max_tokens=max_tokens,
            )
            
            full_response = ""
            async for chunk in stream:
                # print("Chunk:", chunk.strip())  # Uncomment náº¿u muá»‘n xem chi tiáº¿t tá»«ng chunk
                chunk_data = json.loads(chunk)
                if chunk_data.get("type") == "deepsearch":
                    full_response += chunk_data["message"]["content"]
            print("Full response:", full_response)

if __name__ == "__main__":
    asyncio.run(test_stream_response_deepsearch())